{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide_on_feature(x, feature_i, threshold):\n",
    "    \"\"\" Split data into two parts based on if x[feature_i]>=threshold\n",
    "    \"\"\"\n",
    "    if isinstance(threshold, int) or isinstance(threshold, float):\n",
    "        split_func = lambda sample: sample[feature_i] >= threshold\n",
    "    else:\n",
    "        split_func = lambda sample: sample[feature_i] == threshold\n",
    "\n",
    "    x1 = np.array([sample for sample in x if split_func(sample)])\n",
    "    x2 = np.array([sample for sample in x if not split_func(sample)])\n",
    "\n",
    "    return np.array([x1, x2])\n",
    "\n",
    "\n",
    "def calculate_variance(x):\n",
    "    \"\"\" Return the variance of the features in dataset X \"\"\"\n",
    "    mean = np.ones(np.shape(x)) * x.mean(0)\n",
    "    n_samples = np.shape(x)[0]\n",
    "    variance = (1 / n_samples) * np.diag((x - mean).T.dot(x - mean))\n",
    "    return variance\n",
    "\n",
    "\n",
    "def calculate_entropy(y):\n",
    "    log2 = lambda x: math.log(x + 1e-15) / math.log(2)\n",
    "    entropy = 0\n",
    "    for label in np.unique(y):\n",
    "        count = y[y == label]\n",
    "        p = count / len(y)\n",
    "        entropy -= p * log2(p)\n",
    "    return entropy\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionNode():\n",
    "    \"\"\"Class that represents decision node or leaf in the tree\n",
    "    Parameters\n",
    "    ----------\n",
    "    feature_i: int\n",
    "        Feature index which will be used to split the data into two branches\n",
    "    threshold: float\n",
    "        Value used to split into two branches\n",
    "    value: float\n",
    "        The class prediction if classification tree, or float value if regression tree.\n",
    "    true_branch: DecisionNode\n",
    "        Left branch node (will have samples for which feature at i >= threshold)\n",
    "    false_branch: DecisionNode\n",
    "        Right branch node (will have samples for which feature at i < threshold)\n",
    "    \"\"\"\n",
    "    def __init__(self, feature_i=None, threshold=None,\n",
    "                 value=None, true_branch=None, false_branch=None):\n",
    "        self.feature_i = feature_i\n",
    "        self.threshold = threshold\n",
    "        self.value = value\n",
    "        self.true_branch = true_branch\n",
    "        self.false_branch = false_branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree(object):\n",
    "    \"\"\"Class to represent a decision tree\n",
    "    Super class of RegressionTree() and ClassificationTree()\n",
    "    Parameters\n",
    "    ----------\n",
    "    min_samples_split: int\n",
    "        Min number of samples needed to make a split\n",
    "    min_impurity: float\n",
    "        Min impurity required to make a split\n",
    "    max_depth: int\n",
    "        Max depth of tree\n",
    "    loss: function\n",
    "        Loss function used to compute impurity for gradient boosted trees\n",
    "    \"\"\"\n",
    "    def __init__(self, min_samples_split=2, min_impurity=1e-7,\n",
    "                 max_depth=float(\"inf\"), loss=None):\n",
    "        self.root = None # Root node of the tree\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.min_impurity = min_impurity\n",
    "        self.max_depth = max_depth\n",
    "\n",
    "        # Attributes set by child classes\n",
    "        # Function to calculate values at leaf node\n",
    "        # (Regression: mean, Classification: majority)\n",
    "        self._leaf_value_calculation = None\n",
    "        # Metric used to calculate impurity \n",
    "        # (Regression: Variance reduction, classification: Info gain(entropy))\n",
    "        self._impurity_calculation = None\n",
    "        # Loss function, used only for gradient boosted trees\n",
    "        self.loss = None \n",
    "        \n",
    "        # Additional attributes\n",
    "        # If y is one-hot encoded (multi-dim) or not\n",
    "        self.one_dim = None\n",
    "    \n",
    "    def fit(self, x, y):\n",
    "        \"\"\"Determine the tree structure\n",
    "        \"\"\"\n",
    "        self.one_dim = len(np.shape(y))==1\n",
    "        self.root = self._build_tree(x, y)\n",
    "    \n",
    "    def predict_value(self, x, tree=None):\n",
    "        \"\"\"Predict the value/class for a single data point (Sample)\n",
    "        Parameters\n",
    "        ----------\n",
    "        tree: Recursive DecisionNode structure\n",
    "            has attribute value at leaf node\n",
    "        \"\"\"\n",
    "        # Recurse through the tree and find which region each sample in x\n",
    "        # belongs to and then assign the corresponding leaf value\n",
    "        # (Regression: mean, Classification: Majority vote)\n",
    "\n",
    "        if tree is None:\n",
    "            tree = self.root\n",
    "        \n",
    "        # Base case: we have reached leaf level\n",
    "        if tree.value is not None:\n",
    "            return tree.value\n",
    "        \n",
    "        # Decide which branch to take, keep recursing\n",
    "        # taking left or right branch based on feature\n",
    "        # value and threshold\n",
    "        feature_value = x[tree.feature_i]\n",
    "        branch = tree.false_branch\n",
    "        if isinstance(feature_value, int) or isinstance(feature_value, float):\n",
    "            branch = tree.true_branch if feature_value >= tree.threshold else tree.false_branch\n",
    "        else:\n",
    "            branch = tree.true_branch if feature_value == tree.threshold else tree.false_branch\n",
    "\n",
    "        # Recurse through the tree until leaf node is found\n",
    "        return self.predict_value(x, branch)\n",
    "    \n",
    "    def predict(self, x):\n",
    "        \"\"\"Compute tree output for datapoints\n",
    "        \"\"\"\n",
    "        return [self.predict_value(sample) for sample in x]\n",
    "\n",
    "    def _build_tree(self, x, y, current_depth=0):\n",
    "        \"\"\"Build the recursive tree structure based on feature, threshold\n",
    "        resulting in most impurity reduction at each level (greedy)\n",
    "        and compute values at each leaf node\n",
    "        \"\"\"\n",
    "        n_samples, n_features = np.shape(X)\n",
    "\n",
    "        largest_impurity = 0\n",
    "        best_criteria = None\n",
    "        best_sets = None\n",
    "\n",
    "        # If y is one-dim, convert to column vector\n",
    "        if self.one_dim:\n",
    "            y = np.expand_dims(y, axis=1)\n",
    "        \n",
    "        # Concatenate x and y\n",
    "        xy = np.concatenate(x, y, axis=1)\n",
    "\n",
    "        # Continue building tree at this split only if\n",
    "        #   number of samples > min_samples_split \n",
    "        #   current_depth <= max_depth\n",
    "        if n_samples >= self.min_samples_split and current_depth <= self.max_depth:\n",
    "\n",
    "            # Iterate through all features to find the feature with best split\n",
    "            for feature_i in range(n_features):\n",
    "\n",
    "                feature_values = np.expand_dims(x[:, feature_i], axis=1)\n",
    "                unique_values = np.unique(feature_values)\n",
    "\n",
    "                # Iterature through all unique values of a given feature\n",
    "                for threshold in unique_values:\n",
    "\n",
    "                    # Calculate impurity (var-red or info-gain for this split)\n",
    "                    xy1, xy2 = divide_on_feature(xy, feature_i, threshold)\n",
    "\n",
    "                    # Ensure that both branches has at least one sample\n",
    "                    if len(xy1)>0 and len(xy2)>0:\n",
    "                        \n",
    "                        # Compute impurity for the split\n",
    "                        y1 = xy1[:, n_features:]\n",
    "                        y2 = xy2[:, n_features:]\n",
    "\n",
    "                        impurity = self._impurity_calculation(y, y1, y2)\n",
    "                        # If current impurity better than best, save this\n",
    "                        if impurity > largest_impurity:\n",
    "                            largest_impurity = impurity\n",
    "                            best_criteria = {\"feature_i\":feature_i, \"threshold\":threshold};\n",
    "                            best_sets = {\n",
    "                                \"leftx\" : xy1[:, :n_features],\n",
    "                                \"lefty\" : xy1[:, n_features:],\n",
    "                                \"rightx\" : xy2[:, :n_features],\n",
    "                                \"righty\" : xy2[:, n_features:]\n",
    "                            }\n",
    "            \n",
    "        # After iterating through all features, threshold values,\n",
    "        # and finding the feature,threshold with best split (largest impurity reduction),\n",
    "        # continue to build tree recursively (left (True) and right (false) branches)\n",
    "        # if the largest_impurity is more than min_impurity parameter set\n",
    "        if largest_impurity > self.min_impurity:\n",
    "            true_branch = self._build_tree(self, best_sets[\"leftx\"], best_sets[\"lefty\"], current_depth+1)\n",
    "            false_branch = self._build_tree(self, best_sets[\"rightx\"], best_sets[\"righty\"], current_depth+1)\n",
    "            return DecisionNode(feature_i=best_criteria[\"feature_i\"], threshold=best_criteria[\"threshold\"],\n",
    "                                true_branch=true_branch, false_branch=false_branch)\n",
    "        \n",
    "        # If largest_impurity is less than min_impurity needed to split further\n",
    "        # then we have reached the leaf node, compute leaf value\n",
    "        leaf_value = self._leaf_value_calculation(y)\n",
    "        return DecisionNode(value=leaf_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegressionTree(DecisionTree):\n",
    "    \"\"\"Class for Regression Tree\n",
    "    Inherits from Decision tree\n",
    "    \"\"\"\n",
    "    def _variance_reduction(y, y1, y2):\n",
    "        total_variance = calculate_variance(y)\n",
    "        left_branch_variance = calculate_variance(y1)\n",
    "        right_branch_variance = calculate_variance(y2)\n",
    "\n",
    "        ratio = len(y1)/len(y)\n",
    "        variance_reduction = total_variance - (ratio*left_branch_variance + (1-ratio)*right_branch_variance)\n",
    "        return sum(variance_reduction)\n",
    "\n",
    "    def _mean_of_y(y):\n",
    "        return np.mean(y, axis=0)\n",
    "\n",
    "    # functions for impurity calculation and leaf value\n",
    "    # calculation need to be assigned before invoking the \n",
    "    # super's fit function\n",
    "    def fit(self, x, y):\n",
    "        self._impurity_calculation = variance_reduction\n",
    "        self._leaf_value_calculation = mean_of_y\n",
    "        super(RegressionTree, self).fit(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationTree(DecisionTree):\n",
    "    \"\"\"Class for classification Tree\n",
    "    \"\"\"\n",
    "\n",
    "    # Define the leaf calculation and impurity reduction\n",
    "    # functions for classification\n",
    "    def _info_gain(y, y1, y2):\n",
    "        # Compute entropies of entire data, left and right splits\n",
    "        total_entropy = calculate_entropy(y)\n",
    "        left_branch_entropy = calculate_entropy(y1)\n",
    "        right_branch_entropy = calculate_entropy(y2)\n",
    "\n",
    "        ratio = len(y1)/len(y)\n",
    "        info_gain = total_gain - (ratio*left_branch_entropy + (1-ratio)*right_branch_entropy)\n",
    "        return info_gain\n",
    "\n",
    "    def _majority_vote(y):\n",
    "        \"\"\"\n",
    "        most_common = None\n",
    "        max_count = 0\n",
    "        for label in np.unique(y):\n",
    "            # Count number of occurences of samples with label\n",
    "            count = len(y[y == label])\n",
    "            if count > max_count:\n",
    "                most_common = label\n",
    "                max_count = count\n",
    "        \"\"\"\n",
    "        most_common = np.argmax([len(y[y==label]) for label in np.unique(y)])\n",
    "        return most_common\n",
    "        \n",
    "\n",
    "    def fit(self, x, y):\n",
    "        self._impurity_calculation = _info_gain\n",
    "        self._leaf_value_calculation = _majority_vote\n",
    "        super(ClassificationTree, self).fit(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Classification Tree --\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name '_info_gain' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-0f4497cd08bf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mClassificationTree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-4ed4bf4fecf5>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_impurity_calculation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_info_gain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_leaf_value_calculation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_majority_vote\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mClassificationTree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name '_info_gain' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print (\"-- Classification Tree --\")\n",
    "\n",
    "data = datasets.load_iris()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4)\n",
    "\n",
    "clf = ClassificationTree()\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print (\"Accuracy:\", accuracy)\n",
    "\n",
    "Plot().plot_in_2d(X_test, y_pred, \n",
    "    title=\"Decision Tree\", \n",
    "    accuracy=accuracy, \n",
    "    legend_labels=data.target_names)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import progressbar\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cmx\n",
    "import matplotlib.colors as colors\n",
    "import numpy as np\n",
    "\n",
    "def standardize(X):\n",
    "    \"\"\" Standardize the dataset X \"\"\"\n",
    "    X_std = X\n",
    "    mean = X.mean(axis=0)\n",
    "    std = X.std(axis=0)\n",
    "    for col in range(np.shape(X)[1]):\n",
    "        if std[col]:\n",
    "            X_std[:, col] = (X_std[:, col] - mean[col]) / std[col]\n",
    "    # X_std = (X - X.mean(axis=0)) / X.std(axis=0)\n",
    "    return X_std\n",
    "\n",
    "def calculate_covariance_matrix(X, Y=None):\n",
    "    \"\"\" Calculate the covariance matrix for the dataset X \"\"\"\n",
    "    if Y is None:\n",
    "        Y = X\n",
    "    n_samples = np.shape(X)[0]\n",
    "    covariance_matrix = (1 / (n_samples-1)) * (X - X.mean(axis=0)).T.dot(Y - Y.mean(axis=0))\n",
    "\n",
    "    return np.array(covariance_matrix, dtype=float)\n",
    " \n",
    "\n",
    "def calculate_correlation_matrix(X, Y=None):\n",
    "    \"\"\" Calculate the correlation matrix for the dataset X \"\"\"\n",
    "    if Y is None:\n",
    "        Y = X\n",
    "    n_samples = np.shape(X)[0]\n",
    "    covariance = (1 / n_samples) * (X - X.mean(0)).T.dot(Y - Y.mean(0))\n",
    "    std_dev_X = np.expand_dims(calculate_std_dev(X), 1)\n",
    "    std_dev_y = np.expand_dims(calculate_std_dev(Y), 1)\n",
    "    correlation_matrix = np.divide(covariance, std_dev_X.dot(std_dev_y.T))\n",
    "\n",
    "    return np.array(correlation_matrix, dtype=float)\n",
    "\n",
    "\n",
    "bar_widgets = [\n",
    "    'Training: ', progressbar.Percentage(), ' ', progressbar.Bar(marker=\"-\", left=\"[\", right=\"]\"),\n",
    "    ' ', progressbar.ETA()\n",
    "]\n",
    "\n",
    "class Plot():\n",
    "    def __init__(self): \n",
    "        self.cmap = plt.get_cmap('viridis')\n",
    "\n",
    "    def _transform(self, X, dim):\n",
    "        covariance = calculate_covariance_matrix(X)\n",
    "        eigenvalues, eigenvectors = np.linalg.eig(covariance)\n",
    "        # Sort eigenvalues and eigenvector by largest eigenvalues\n",
    "        idx = eigenvalues.argsort()[::-1]\n",
    "        eigenvalues = eigenvalues[idx][:dim]\n",
    "        eigenvectors = np.atleast_1d(eigenvectors[:, idx])[:, :dim]\n",
    "        # Project the data onto principal components\n",
    "        X_transformed = X.dot(eigenvectors)\n",
    "\n",
    "        return X_transformed\n",
    "\n",
    "\n",
    "    def plot_regression(self, lines, title, axis_labels=None, mse=None, scatter=None, legend={\"type\": \"lines\", \"loc\": \"lower right\"}):\n",
    "        \n",
    "        if scatter:\n",
    "            scatter_plots = scatter_labels = []\n",
    "            for s in scatter:\n",
    "                scatter_plots += [plt.scatter(s[\"x\"], s[\"y\"], color=s[\"color\"], s=s[\"size\"])]\n",
    "                scatter_labels += [s[\"label\"]]\n",
    "            scatter_plots = tuple(scatter_plots)\n",
    "            scatter_labels = tuple(scatter_labels)\n",
    "\n",
    "        for l in lines:\n",
    "            li = plt.plot(l[\"x\"], l[\"y\"], color=s[\"color\"], linewidth=l[\"width\"], label=l[\"label\"])\n",
    "\n",
    "        if mse:\n",
    "            plt.suptitle(title)\n",
    "            plt.title(\"MSE: %.2f\" % mse, fontsize=10)\n",
    "        else:\n",
    "            plt.title(title)\n",
    "\n",
    "        if axis_labels:\n",
    "            plt.xlabel(axis_labels[\"x\"])\n",
    "            plt.ylabel(axis_labels[\"y\"])\n",
    "\n",
    "        if legend[\"type\"] == \"lines\":\n",
    "            plt.legend(loc=\"lower_left\")\n",
    "        elif legend[\"type\"] == \"scatter\" and scatter:\n",
    "            plt.legend(scatter_plots, scatter_labels, loc=legend[\"loc\"])\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "\n",
    "    # Plot the dataset X and the corresponding labels y in 2D using PCA.\n",
    "    def plot_in_2d(self, X, y=None, title=None, accuracy=None, legend_labels=None):\n",
    "        X_transformed = self._transform(X, dim=2)\n",
    "        x1 = X_transformed[:, 0]\n",
    "        x2 = X_transformed[:, 1]\n",
    "        class_distr = []\n",
    "\n",
    "        y = np.array(y).astype(int)\n",
    "\n",
    "        colors = [self.cmap(i) for i in np.linspace(0, 1, len(np.unique(y)))]\n",
    "\n",
    "        # Plot the different class distributions\n",
    "        for i, l in enumerate(np.unique(y)):\n",
    "            _x1 = x1[y == l]\n",
    "            _x2 = x2[y == l]\n",
    "            _y = y[y == l]\n",
    "            class_distr.append(plt.scatter(_x1, _x2, color=colors[i]))\n",
    "\n",
    "        # Plot legend\n",
    "        if not legend_labels is None: \n",
    "            plt.legend(class_distr, legend_labels, loc=1)\n",
    "\n",
    "        # Plot title\n",
    "        if title:\n",
    "            if accuracy:\n",
    "                perc = 100 * accuracy\n",
    "                plt.suptitle(title)\n",
    "                plt.title(\"Accuracy: %.1f%%\" % perc, fontsize=10)\n",
    "            else:\n",
    "                plt.title(title)\n",
    "\n",
    "        # Axis labels\n",
    "        plt.xlabel('Principal Component 1')\n",
    "        plt.ylabel('Principal Component 2')\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "    # Plot the dataset X and the corresponding labels y in 3D using PCA.\n",
    "    def plot_in_3d(self, X, y=None):\n",
    "        X_transformed = self._transform(X, dim=3)\n",
    "        x1 = X_transformed[:, 0]\n",
    "        x2 = X_transformed[:, 1]\n",
    "        x3 = X_transformed[:, 2]\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "        ax.scatter(x1, x2, x3, c=y)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 0, 1, 1, 2])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([1, 2, 0, 1, 1, 2])\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.bincount(a).argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.random.randint(0,3, (10,1))\n",
    "np.unique(y)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax([len(y[y==label]) for label in np.unique(y)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6405b044df7d297575459db63fc228bdb9d95a12369b8249bfbb6c77cae13e4b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
