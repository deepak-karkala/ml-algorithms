{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "kGOciRwGBthN"
      },
      "outputs": [],
      "source": [
        "from __future__ import unicode_literals, print_function, division\n",
        "from io import open\n",
        "import unicodedata\n",
        "import re\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import numpy as np\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Helper functions"
      ],
      "metadata": {
        "id": "p2dm9NEiHEEy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import math\n",
        "\n",
        "def asMinutes(s):\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "def timeSince(since, percent):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    es = s / (percent)\n",
        "    rs = es - s\n",
        "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.switch_backend('agg')\n",
        "import matplotlib.ticker as ticker\n",
        "import numpy as np\n",
        "\n",
        "def showPlot(points):\n",
        "    plt.figure()\n",
        "    fig, ax = plt.subplots()\n",
        "    # this locator puts ticks at regular intervals\n",
        "    loc = ticker.MultipleLocator(base=0.2)\n",
        "    ax.yaxis.set_major_locator(loc)\n",
        "    plt.plot(points)"
      ],
      "metadata": {
        "id": "8lJdRkoBHQue"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Uploading Data"
      ],
      "metadata": {
        "id": "xDsoZ8t3HEP5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "from google.colab import files\n",
        "#drive.mount('/content/drive')\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"Uploading_Data_Colab_1.xlsx\" with length 9000 bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))\n",
        "\"\"\"\n",
        "\n",
        "!wget -cq https://download.pytorch.org/tutorial/data.zip\n",
        "\n",
        "import zipfile\n",
        "with zipfile.ZipFile('data.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('.')"
      ],
      "metadata": {
        "id": "VFYIaS17HUeI"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNVF_RHuBthP"
      },
      "source": [
        "## Loading and pre-processing data files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EDGGk20WBthT",
        "outputId": "1cc7d52a-be1e-478e-f442-24d1b31cd092"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading lines...\n",
            "Read 135842 sentence pairs\n",
            "Trimmed to 11445 sentence pairs\n",
            "Counting words...\n",
            "Counted words:\n",
            "fra 4601\n",
            "eng 2991\n",
            "['je suis comme ma mere', 'i am like my mother']\n"
          ]
        }
      ],
      "source": [
        "SOS_token = 0\n",
        "EOS_token = 1\n",
        "\n",
        "class Lang:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.word2index = {}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
        "        self.n_words = 2  # Count SOS and EOS\n",
        "\n",
        "    def addSentence(self, sentence):\n",
        "        for word in sentence.split(' '):\n",
        "            self.addWord(word)\n",
        "\n",
        "    def addWord(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.n_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.n_words] = word\n",
        "            self.n_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1\n",
        "\n",
        "# Turn a Unicode string to plain ASCII, thanks to\n",
        "# https://stackoverflow.com/a/518232/2809427\n",
        "def unicodeToAscii(s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "    )\n",
        "\n",
        "# Lowercase, trim, and remove non-letter characters\n",
        "def normalizeString(s):\n",
        "    s = unicodeToAscii(s.lower().strip())\n",
        "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "    s = re.sub(r\"[^a-zA-Z!?]+\", r\" \", s)\n",
        "    return s.strip()\n",
        "\n",
        "\n",
        "def readLangs(lang1, lang2, reverse=False):\n",
        "    print(\"Reading lines...\")\n",
        "\n",
        "    # Read the file and split into lines\n",
        "    lines = open('data/%s-%s.txt' % (lang1, lang2), encoding='utf-8').\\\n",
        "        read().strip().split('\\n')\n",
        "\n",
        "    # Split every line into pairs and normalize\n",
        "    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
        "\n",
        "    # Reverse pairs, make Lang instances\n",
        "    if reverse:\n",
        "        pairs = [list(reversed(p)) for p in pairs]\n",
        "        input_lang = Lang(lang2)\n",
        "        output_lang = Lang(lang1)\n",
        "    else:\n",
        "        input_lang = Lang(lang1)\n",
        "        output_lang = Lang(lang2)\n",
        "\n",
        "    return input_lang, output_lang, pairs\n",
        "\n",
        "\n",
        "MAX_LENGTH = 10\n",
        "\n",
        "eng_prefixes = (\n",
        "    \"i am \", \"i m \",\n",
        "    \"he is\", \"he s \",\n",
        "    \"she is\", \"she s \",\n",
        "    \"you are\", \"you re \",\n",
        "    \"we are\", \"we re \",\n",
        "    \"they are\", \"they re \"\n",
        ")\n",
        "\n",
        "def filterPair(p):\n",
        "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
        "        len(p[1].split(' ')) < MAX_LENGTH and \\\n",
        "        p[1].startswith(eng_prefixes)\n",
        "\n",
        "\n",
        "def filterPairs(pairs):\n",
        "    return [pair for pair in pairs if filterPair(pair)]\n",
        "\n",
        "\n",
        "def prepareData(lang1, lang2, reverse=False):\n",
        "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
        "    print(\"Read %s sentence pairs\" % len(pairs))\n",
        "    pairs = filterPairs(pairs)\n",
        "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
        "    print(\"Counting words...\")\n",
        "    for pair in pairs:\n",
        "        input_lang.addSentence(pair[0])\n",
        "        output_lang.addSentence(pair[1])\n",
        "    print(\"Counted words:\")\n",
        "    print(input_lang.name, input_lang.n_words)\n",
        "    print(output_lang.name, output_lang.n_words)\n",
        "    return input_lang, output_lang, pairs\n",
        "\n",
        "input_lang, output_lang, pairs = prepareData('eng', 'fra', True)\n",
        "print(random.choice(pairs))\n",
        "\n",
        "def indexesFromSentence(lang, sentence):\n",
        "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
        "\n",
        "def tensorFromSentence(lang, sentence):\n",
        "    indexes = indexesFromSentence(lang, sentence)\n",
        "    indexes.append(EOS_token)\n",
        "    return torch.tensor(indexes, dtype=torch.long, device=device).view(1, -1)\n",
        "\n",
        "def tensorsFromPair(pair):\n",
        "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
        "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
        "    return (input_tensor, target_tensor)\n",
        "\n",
        "def get_dataloader(batch_size):\n",
        "    input_lang, output_lang, pairs = prepareData('eng', 'fra', True)\n",
        "\n",
        "    n = len(pairs)\n",
        "    input_ids = np.zeros((n, MAX_LENGTH), dtype=np.int32)\n",
        "    target_ids = np.zeros((n, MAX_LENGTH), dtype=np.int32)\n",
        "\n",
        "    for idx, (inp, tgt) in enumerate(pairs):\n",
        "        inp_ids = indexesFromSentence(input_lang, inp)\n",
        "        tgt_ids = indexesFromSentence(output_lang, tgt)\n",
        "        inp_ids.append(EOS_token)\n",
        "        tgt_ids.append(EOS_token)\n",
        "        input_ids[idx, :len(inp_ids)] = inp_ids\n",
        "        target_ids[idx, :len(tgt_ids)] = tgt_ids\n",
        "\n",
        "    train_data = TensorDataset(torch.LongTensor(input_ids).to(device),\n",
        "                               torch.LongTensor(target_ids).to(device))\n",
        "\n",
        "    train_sampler = RandomSampler(train_data)\n",
        "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "    return input_lang, output_lang, train_dataloader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8fz9b8VBthX"
      },
      "source": [
        "### Encoder Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "GUsyOwYNBthX"
      },
      "outputs": [],
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, dropout=0.1):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, input):\n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "        output, hidden = self.gru(embedded)\n",
        "        return output, hidden"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rP6GtSQ2BthX"
      },
      "source": [
        "### Decoder with Attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "A4Gq_-E2BthY"
      },
      "outputs": [],
      "source": [
        "class BahdanauAttention(nn.Module):\n",
        "    def __init__(self, dim_attention, dim_keys, dim_query):\n",
        "        super(BahdanauAttention, self).__init__()\n",
        "        self.Wa = nn.Linear(dim_attention, dim_query)\n",
        "        self.Ua = nn.Linear(dim_attention, dim_keys)\n",
        "        self.Va = nn.Linear(dim_attention, 1)\n",
        "\n",
        "    def forward(self, query, keys, values):\n",
        "        scores = self.Va(torch.tanh(self.Wa(query) + self.Ua(keys)))\n",
        "        scores = scores.squeeze(2).unsqueeze(1)\n",
        "\n",
        "        weights = F.softmax(scores, dim=-1)\n",
        "        context = torch.bmm(weights, values)\n",
        "\n",
        "        return context, weights\n",
        "\n",
        "\n",
        "class AttentionDecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size, dropout=0.1):\n",
        "        super(AttentionDecoderRNN, self).__init__()\n",
        "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "        self.attention = BahdanauAttention(hidden_size, hidden_size, hidden_size)\n",
        "        self.gru = nn.GRU(2*hidden_size, hidden_size, batch_first=True)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward_step(self, input, hidden, encoder_outputs):\n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "\n",
        "        context, attention_weights = self.attention(query=hidden.permute(1, 0, 2), keys=encoder_outputs, values=encoder_outputs)\n",
        "\n",
        "        input_gru = torch.cat((embedded, context), dim=2)\n",
        "        output, hidden = self.gru(input_gru, hidden)\n",
        "        output = self.out(output)\n",
        "\n",
        "        return output, hidden, attention_weights\n",
        "\n",
        "    def forward(self, encoder_outputs, encoder_hidden, target_tensor=None):\n",
        "        batch_size = encoder_outputs.size(0)\n",
        "        decoder_input = torch.empty(batch_size, 1, dtype=torch.long, device=device).fill_(SOS_token)\n",
        "        decoder_hidden= encoder_hidden\n",
        "        decoder_outputs = []\n",
        "        attentions = []\n",
        "\n",
        "        # Run the decoder for MAX_LENGTH number of steps\n",
        "        for i in range(MAX_LENGTH):\n",
        "            # One step of decoder\n",
        "            decoder_output, decoder_hidden, attention_weight = self.forward_step(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            decoder_outputs.append(decoder_output)\n",
        "            attentions.append(attention_weight)\n",
        "\n",
        "            if target_tensor is not None:\n",
        "                # Teacher forcing: using target as next input\n",
        "                decoder_input = target_tensor[:, i].unsqueeze(1)\n",
        "            else:\n",
        "                # Using decoder output in this step as input to next step\n",
        "                _, topi = decoder_output.topk(1)\n",
        "                decoder_input = topi.squeeze(-1).detach()\n",
        "\n",
        "        decoder_outputs = torch.cat(decoder_outputs, dim=1)\n",
        "        decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)\n",
        "        attentions = torch.cat(attentions, dim=1)\n",
        "\n",
        "        return decoder_outputs, decoder_hidden, attentions\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GVXFwySkgDvC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Seq2Seq Model"
      ],
      "metadata": {
        "id": "j4Icll7ALP0f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class seq2seqAttention(nn.Module):\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super(seq2seqAttention, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "    def forward(self, input_tensor, target_tensor):\n",
        "        # Forward propagation through encoder and decoder (one batch, MAX_LENGTH time steps)\n",
        "        encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
        "        decoder_outputs, decoder_hidden, attentions = decoder(encoder_outputs, encoder_hidden, target_tensor)\n",
        "        return decoder_outputs, decoder_hidden, attentions"
      ],
      "metadata": {
        "id": "4JZpseQbLOtv"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lgEdbX7kBthY"
      },
      "source": [
        "### Training script"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lf71nvTZBthZ"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "def train(train_dataloader, encoder, decoder, n_epochs, learning_rate=0.001, print_every=100, plot_every=100):\n",
        "    start = time.time()\n",
        "    plot_losses = []\n",
        "    print_loss_total = 0  # Reset every print_every\n",
        "    plot_loss_total = 0  # Reset every plot_every\n",
        "\n",
        "    # Optimizers and loss function\n",
        "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
        "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
        "    criterion = nn.NLLLoss()\n",
        "\n",
        "    for epoch in range(1, n_epochs + 1):\n",
        "        # Train network for on epoch\n",
        "        loss = train_epoch(train_dataloader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
        "        print_loss_total += loss\n",
        "        plot_loss_total += loss\n",
        "\n",
        "        if epoch % print_every == 0:\n",
        "            print_loss_avg = print_loss_total / print_every\n",
        "            print_loss_total = 0\n",
        "            print('%s (%d %d%%) %.4f' % (timeSince(start, epoch / n_epochs),\n",
        "                                        epoch, epoch / n_epochs * 100, print_loss_avg))\n",
        "\n",
        "        if epoch % plot_every == 0:\n",
        "            plot_loss_avg = plot_loss_total / plot_every\n",
        "            plot_losses.append(plot_loss_avg)\n",
        "            plot_loss_total = 0\n",
        "\n",
        "    showPlot(plot_losses)\n",
        "\n",
        "def train_epoch(dataloader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion):\n",
        "    total_loss = 0\n",
        "\n",
        "    for data in dataloader:\n",
        "        input_tensor, target_tensor = data\n",
        "\n",
        "        # Reset optimizers\n",
        "        encoder_optimizer.zero_grad()\n",
        "        decoder_optimizer.zero_grad()\n",
        "\n",
        "        # Forward propagation through encoder and decoder (one batch, MAX_LENGTH time steps)\n",
        "        encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
        "        decoder_outputs, _, _ = decoder(encoder_outputs, encoder_hidden, target_tensor)\n",
        "\n",
        "        # Compute loss: NLL\n",
        "        loss = criterion(\n",
        "            decoder_outputs.view(-1, decoder_outputs.size(-1)),\n",
        "            target_tensor.view(-1)\n",
        "        )\n",
        "        # Back propagate gradients\n",
        "        loss.backward()\n",
        "\n",
        "        # Update network parameters\n",
        "        encoder_optimizer.step()\n",
        "        decoder_optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(dataloader)\n",
        "\"\"\"\n",
        "\n",
        "def train(train_dataloader, model, n_epochs, learning_rate=0.001, print_every=100, plot_every=100):\n",
        "    start = time.time()\n",
        "    plot_losses = []\n",
        "    print_loss_total = 0  # Reset every print_every\n",
        "    plot_loss_total = 0  # Reset every plot_every\n",
        "\n",
        "    # Optimizers and loss function\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    criterion = nn.NLLLoss()\n",
        "\n",
        "    for epoch in range(1, n_epochs + 1):\n",
        "        # Train network for on epoch\n",
        "        loss = train_epoch(train_dataloader, model, optimizer, criterion)\n",
        "        print_loss_total += loss\n",
        "        plot_loss_total += loss\n",
        "\n",
        "        if epoch % print_every == 0:\n",
        "            print_loss_avg = print_loss_total / print_every\n",
        "            print_loss_total = 0\n",
        "            print('%s (%d %d%%) %.4f' % (timeSince(start, epoch / n_epochs),\n",
        "                                        epoch, epoch / n_epochs * 100, print_loss_avg))\n",
        "\n",
        "        if epoch % plot_every == 0:\n",
        "            plot_loss_avg = plot_loss_total / plot_every\n",
        "            plot_losses.append(plot_loss_avg)\n",
        "            plot_loss_total = 0\n",
        "\n",
        "    showPlot(plot_losses)\n",
        "\n",
        "def train_epoch(dataloader, model, optimizer, criterion):\n",
        "    total_loss = 0\n",
        "\n",
        "    for data in dataloader:\n",
        "        input_tensor, target_tensor = data\n",
        "\n",
        "        # Reset optimizers\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward propagation through encoder and decoder (one batch, MAX_LENGTH time steps)\n",
        "        decoder_outputs, _, _ = model(input_tensor, target_tensor)\n",
        "\n",
        "        # Compute loss: NLL\n",
        "        y = target_tensor.view(-1)\n",
        "        y_pred = decoder_outputs.view(-1, decoder_outputs.size(-1))\n",
        "        loss = criterion(y_pred, y)\n",
        "        # Back propagate gradients\n",
        "        loss.backward()\n",
        "\n",
        "        # Update network parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(dataloader)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train the network"
      ],
      "metadata": {
        "id": "wlzS7R8bKaPO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_size = 128\n",
        "batch_size = 32\n",
        "\n",
        "input_lang, output_lang, train_dataloader = get_dataloader(batch_size)\n",
        "\n",
        "encoder = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
        "decoder = AttentionDecoderRNN(hidden_size, output_lang.n_words).to(device)\n",
        "model = seq2seqAttention(encoder, decoder).to(device)"
      ],
      "metadata": {
        "id": "u0ORy0kAORIL",
        "outputId": "9dda71e3-aced-4522-9040-2dc9c8b33631",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading lines...\n",
            "Read 135842 sentence pairs\n",
            "Trimmed to 11445 sentence pairs\n",
            "Counting words...\n",
            "Counted words:\n",
            "fra 4601\n",
            "eng 2991\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b07WfNOYBthZ"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "#train(train_dataloader, encoder, decoder, 80, print_every=5, plot_every=5)\n",
        "train(train_dataloader, model, 1, print_every=5, plot_every=5)"
      ],
      "metadata": {
        "id": "lEIocPUyKqjL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(encoder, decoder, sentence, input_lang, output_lang):\n",
        "    with torch.no_grad():\n",
        "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
        "\n",
        "        encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
        "        decoder_outputs, decoder_hidden, decoder_attn = decoder(encoder_outputs, encoder_hidden)\n",
        "\n",
        "        _, topi = decoder_outputs.topk(1)\n",
        "        decoded_ids = topi.squeeze()\n",
        "\n",
        "        decoded_words = []\n",
        "        for idx in decoded_ids:\n",
        "            if idx.item() == EOS_token:\n",
        "                decoded_words.append('<EOS>')\n",
        "                break\n",
        "            decoded_words.append(output_lang.index2word[idx.item()])\n",
        "    return decoded_words, decoder_attn\n",
        "\n",
        "def evaluateRandomly(encoder, decoder, n=10):\n",
        "    for i in range(n):\n",
        "        pair = random.choice(pairs)\n",
        "        print('>', pair[0])\n",
        "        print('=', pair[1])\n",
        "        output_words, _ = evaluate(encoder, decoder, pair[0], input_lang, output_lang)\n",
        "        output_sentence = ' '.join(output_words)\n",
        "        print('<', output_sentence)\n",
        "        print('')\n",
        "\n",
        "encoder.eval()\n",
        "decoder.eval()\n",
        "evaluateRandomly(encoder, decoder)"
      ],
      "metadata": {
        "id": "Ce9qgkkwK1xk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PyTorch lightning"
      ],
      "metadata": {
        "id": "JkEohURbFyrK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install lightning\n",
        "import lightning as L"
      ],
      "metadata": {
        "id": "VZP0SM3GF1V4",
        "outputId": "98361a7b-9a44-4cce-d481-65115d0d8904",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting lightning\n",
            "  Downloading lightning-2.2.0-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML<8.0,>=5.4 in /usr/local/lib/python3.10/dist-packages (from lightning) (6.0.1)\n",
            "Requirement already satisfied: fsspec[http]<2025.0,>=2022.5.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (2023.6.0)\n",
            "Collecting lightning-utilities<2.0,>=0.8.0 (from lightning)\n",
            "  Downloading lightning_utilities-0.10.1-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.17.2 in /usr/local/lib/python3.10/dist-packages (from lightning) (1.23.5)\n",
            "Requirement already satisfied: packaging<25.0,>=20.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (23.2)\n",
            "Requirement already satisfied: torch<4.0,>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (2.1.0+cu121)\n",
            "Collecting torchmetrics<3.0,>=0.7.0 (from lightning)\n",
            "  Downloading torchmetrics-1.3.0.post0-py3-none-any.whl (840 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m840.2/840.2 kB\u001b[0m \u001b[31m61.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm<6.0,>=4.57.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions<6.0,>=4.4.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (4.9.0)\n",
            "Collecting pytorch-lightning (from lightning)\n",
            "  Downloading pytorch_lightning-2.2.0-py3-none-any.whl (800 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m800.3/800.3 kB\u001b[0m \u001b[31m62.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<2025.0,>=2022.5.0->lightning) (2.31.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<2025.0,>=2022.5.0->lightning) (3.9.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities<2.0,>=0.8.0->lightning) (67.7.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=1.13.0->lightning) (3.13.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=1.13.0->lightning) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=1.13.0->lightning) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=1.13.0->lightning) (3.1.3)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=1.13.0->lightning) (2.1.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning) (4.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<4.0,>=1.13.0->lightning) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]<2025.0,>=2022.5.0->lightning) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]<2025.0,>=2022.5.0->lightning) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]<2025.0,>=2022.5.0->lightning) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]<2025.0,>=2022.5.0->lightning) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch<4.0,>=1.13.0->lightning) (1.3.0)\n",
            "Installing collected packages: lightning-utilities, torchmetrics, pytorch-lightning, lightning\n",
            "Successfully installed lightning-2.2.0 lightning-utilities-0.10.1 pytorch-lightning-2.2.0 torchmetrics-1.3.0.post0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LitSeq2seqAttention(L.LightningModule):\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = optim.Adam(self.parameters(), lr=1e-3)\n",
        "        return optimizer\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        # training_step defines the train loop.\n",
        "        x, y = batch\n",
        "        encoder_outputs, encoder_hidden = encoder(x)\n",
        "        decoder_outputs, decoder_hidden, attentions = decoder(encoder_outputs, encoder_hidden)\n",
        "        logits = decoder_outputs.view(-1, decoder_outputs.size(-1))\n",
        "        loss_fn = nn.NLLLoss()\n",
        "        loss = loss_fn(logits, y.view(-1))\n",
        "        return loss\n",
        "    \"\"\"\n",
        "    def backward(self, trainer, loss, optimizer, optimizer_idx):\n",
        "        loss.backward()\n",
        "\n",
        "    def optimizer_step(self, epoch, batch_idx, optimizer, optimizer_idx):\n",
        "        optimizer.step()\n",
        "    \"\"\""
      ],
      "metadata": {
        "id": "hCzPwUgJF1c6"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# init the autoencoder\n",
        "modelLit = LitSeq2seqAttention(encoder, decoder)\n",
        "trainer = L.Trainer(limit_train_batches=100, max_epochs=10)\n",
        "trainer.fit(model=modelLit, train_dataloaders=train_dataloader)"
      ],
      "metadata": {
        "id": "DhIqYB_oIX7m",
        "outputId": "fade7abc-c3e8-4909-8e83-07f25cc54dd6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 625,
          "referenced_widgets": [
            "dced64af9f5c4258a358038847cef2ab",
            "92b6868e4aef4ebca649c15a5fa2c0fb",
            "3c0aee9082c54f6ea6e6eba8b9b2656a",
            "622e5f388cf64bb6be8adb7a73b7bf38",
            "3f3911b1383e46879e12ed20f9090a87",
            "e8e47a3959db473b92dd329c251f1ad8",
            "e17250f8fccf4fddb9c3fc15a26f391e",
            "93237a3f94a9459ca09ea2a62dd39b79",
            "73f9d0d8a62940c183aa96b3b7b1efa3",
            "1a56b5bb8ac3476a94a3160812ead787",
            "e5ea797b09a343cb8cf7a3d8633260a3"
          ]
        }
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO: GPU available: True (cuda), used: True\n",
            "INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO: TPU available: False, using: 0 TPU cores\n",
            "INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO: IPU available: False, using: 0 IPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO: HPU available: False, using: 0 HPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO: \n",
            "  | Name    | Type                | Params\n",
            "------------------------------------------------\n",
            "0 | encoder | EncoderRNN          | 688 K \n",
            "1 | decoder | AttentionDecoderRNN | 950 K \n",
            "------------------------------------------------\n",
            "1.6 M     Trainable params\n",
            "0         Non-trainable params\n",
            "1.6 M     Total params\n",
            "6.552     Total estimated model params size (MB)\n",
            "INFO:lightning.pytorch.callbacks.model_summary:\n",
            "  | Name    | Type                | Params\n",
            "------------------------------------------------\n",
            "0 | encoder | EncoderRNN          | 688 K \n",
            "1 | decoder | AttentionDecoderRNN | 950 K \n",
            "------------------------------------------------\n",
            "1.6 M     Trainable params\n",
            "0         Non-trainable params\n",
            "1.6 M     Total params\n",
            "6.552     Total estimated model params size (MB)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Training: |          | 0/? [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dced64af9f5c4258a358038847cef2ab"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO: `Trainer.fit` stopped: `max_epochs=10` reached.\n",
            "INFO:lightning.pytorch.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=10` reached.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## With Keras model.fit()\n"
      ],
      "metadata": {
        "id": "7jhFO_WIO82Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# This guide can only be run with the torch backend.\n",
        "os.environ[\"KERAS_BACKEND\"] = \"torch\"\n",
        "\n",
        "import keras\n",
        "from keras import layers"
      ],
      "metadata": {
        "id": "mfL_VbHJSWx3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Seq2seqAttentionKeras(keras.Model):\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super(seq2seqAttentionKeras, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "    def call(self, input_tensor, training=True):\n",
        "        # Forward propagation through encoder and decoder (one batch, MAX_LENGTH time steps)\n",
        "        print(input_tensor.type)\n",
        "        encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
        "        decoder_outputs, decoder_hidden, attentions = decoder(encoder_outputs, encoder_hidden)\n",
        "        return decoder_outputs, decoder_hidden, attentions\n",
        "\n",
        "    def train_step(self, data):\n",
        "        for step, (inputs, targets) in enumerate(data):\n",
        "            # Forward pass\n",
        "            #decoder_outputs, _, _ = self(inputs)\n",
        "            encoder_outputs, encoder_hidden = encoder(inputs)\n",
        "            decoder_outputs, decoder_hidden, attentions = decoder(encoder_outputs, encoder_hidden)\n",
        "            logits = decoder_outputs.view(-1, decoder_outputs.size(-1))\n",
        "            loss = loss_fn(logits, targets.view(-1))\n",
        "\n",
        "            # Backward pass\n",
        "            modelKeras.zero_grad()\n",
        "            trainable_weights = [v for v in modelKeras.trainable_weights]\n",
        "\n",
        "            # Backpropagate loss to compute gradients for the weights.\n",
        "            loss.backward()\n",
        "            gradients = [v.value.grad for v in trainable_weights]\n",
        "\n",
        "            # Update weights\n",
        "            with torch.no_grad():\n",
        "                optimizer.apply(gradients, trainable_weights)\n",
        "\n",
        "            \"\"\"\n",
        "            # Log every 100 batches.\n",
        "            if step % 100 == 0:\n",
        "                print(\n",
        "                    f\"Training loss (for 1 batch) at step {step}: {loss.detach().cpu().numpy():.4f}\"\n",
        "                )\n",
        "                print(f\"Seen so far: {(step + 1) * batch_size} samples\")\n",
        "            \"\"\"\n",
        "\n",
        "        \"\"\"\n",
        "    def train_step(self, data):\n",
        "        # Unpack the data. Its structure depends on your model and\n",
        "        # on what you pass to `fit()`.\n",
        "        x, y = data\n",
        "\n",
        "        # Call torch.nn.Module.zero_grad() to clear the leftover gradients\n",
        "        # for the weights from the previous train step.\n",
        "        self.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        # encoder_outputs, encoder_hidden = self.encoder(x)\n",
        "        # decoder_outputs, decoder_hidden, attentions = self.decoder(encoder_outputs, encoder_hidden, y)\n",
        "        decoder_outputs, decoder_hidden, attentions = self(x.to(torch.int64), training=True)\n",
        "        y_pred = decoder_outputs.view(-1, decoder_outputs.size(-1))\n",
        "\n",
        "        # Compute loss\n",
        "        loss = self.compute_loss(y=y, y_pred=y_pred)\n",
        "\n",
        "        # Call torch.Tensor.backward() on the loss to compute gradients\n",
        "        # for the weights.\n",
        "        loss.backward()\n",
        "\n",
        "        trainable_weights = [v for v in self.trainable_weights]\n",
        "        gradients = [v.value.grad for v in trainable_weights]\n",
        "\n",
        "        # Update weights\n",
        "        with torch.no_grad():\n",
        "            self.optimizer.apply(gradients, trainable_weights)\n",
        "        \"\"\"\n",
        "        \"\"\"\n",
        "        # Update metrics (includes the metric that tracks the loss)\n",
        "        for metric in self.metrics:\n",
        "            if metric.name == \"loss\":\n",
        "                metric.update_state(loss)\n",
        "            else:\n",
        "                metric.update_state(y, y_pred)\n",
        "\n",
        "        # Return a dict mapping metric names to current value\n",
        "        # Note that it will include the loss (tracked in self.metrics).\n",
        "        return {m.name: m.result() for m in self.metrics}\n",
        "\n",
        "        return loss\n",
        "        \"\"\"\n"
      ],
      "metadata": {
        "id": "2y1bjK5fO4LB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_lang, output_lang, train_dataloader = get_dataloader(batch_size)\n",
        "\n",
        "encoder = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
        "decoder = AttentionDecoderRNN(hidden_size, output_lang.n_words).to(device)\n",
        "modelKeras = seq2seqAttentionKeras(encoder, decoder).to(device)\n",
        "\n",
        "loss_fn = nn.NLLLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "for epoch in range(10):\n",
        "    for step, (inputs, targets) in enumerate(train_dataloader):\n",
        "        # Forward pass\n",
        "        decoder_outputs, _, _ = modelKeras(inputs)\n",
        "        logits = decoder_outputs.view(-1, decoder_outputs.size(-1))\n",
        "        loss = loss_fn(logits, targets.view(-1))\n",
        "\n",
        "        # Backward pass\n",
        "        model.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        # Optimizer variable updates\n",
        "        optimizer.step()"
      ],
      "metadata": {
        "id": "KPGEyWWwdlgE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_lang, output_lang, train_dataloader = get_dataloader(batch_size)\n",
        "\n",
        "encoder = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
        "decoder = AttentionDecoderRNN(hidden_size, output_lang.n_words).to(device)\n",
        "modelKeras = seq2seqAttentionKeras(encoder, decoder).to(device)\n",
        "\n",
        "loss_fn = nn.NLLLoss() #keras.losses.CategoricalCrossentropy(from_logits=False)\n",
        "optimizer = keras.optimizers.Adam(learning_rate=1e-3)\n",
        "\n",
        "for epoch in range(10):\n",
        "    print(f\"\\nStart of epoch {epoch}\")\n",
        "    for step, (inputs, targets) in enumerate(train_dataloader):\n",
        "        # Forward pass\n",
        "        decoder_outputs, _, _ = modelKeras(inputs)\n",
        "        logits = decoder_outputs.view(-1, decoder_outputs.size(-1))\n",
        "        loss = loss_fn(logits, targets.view(-1))\n",
        "\n",
        "        # Backward pass\n",
        "        modelKeras.zero_grad()\n",
        "        trainable_weights = [v for v in modelKeras.trainable_weights]\n",
        "\n",
        "        # Call torch.Tensor.backward() on the loss to compute gradients\n",
        "        # for the weights.\n",
        "        loss.backward()\n",
        "        gradients = [v.value.grad for v in trainable_weights]\n",
        "\n",
        "        # Update weights\n",
        "        with torch.no_grad():\n",
        "            optimizer.apply(gradients, trainable_weights)\n",
        "\n",
        "        # Log every 100 batches.\n",
        "        if step % 100 == 0:\n",
        "            print(\n",
        "                f\"Training loss (for 1 batch) at step {step}: {loss.detach().cpu().numpy():.4f}\"\n",
        "            )\n",
        "            print(f\"Seen so far: {(step + 1) * batch_size} samples\")"
      ],
      "metadata": {
        "id": "Abw-14qfi6fE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modelKeras = seq2seqAttentionKeras(encoder, decoder).to(device)\n",
        "input, target = next(iter(train_dataloader))\n",
        "modelKeras(input)\n",
        "\n",
        "modelKeras.compile(optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
        "              loss=nn.NLLLoss(),\n",
        "              metrics=[keras.metrics.CategoricalAccuracy()])"
      ],
      "metadata": {
        "id": "NCIhpa5jRkl3",
        "outputId": "99571479-506a-4a94-b9e2-065a04e743b0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<built-in method type of Tensor object at 0x7ff16bd29670>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history = modelKeras.fit(\n",
        "    train_dataloader,\n",
        "    epochs=1,\n",
        "    steps_per_epoch = 1,\n",
        "    validation_data=train_dataloader,\n",
        "    validation_steps = 1,\n",
        ") #callbacks=[tf.keras.callbacks.EarlyStopping(patience=3)])"
      ],
      "metadata": {
        "id": "YjgZ89toW5u-",
        "outputId": "b7e78049-9ce8-409a-adf3-b28286ffcbd1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 600
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<built-in method type of Tensor object at 0x7ff16bd0e9d0>\n",
            "<built-in method type of Tensor object at 0x7ff16bd510d0>\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Unable to automatically build the model. Please build it yourself before calling fit/evaluate/predict. A model is 'built' when its variables have been created and its `self.built` attribute is True. Usually, calling the model on a batch of data is the right way to build it.\nException encountered:\n'Exception encountered when calling seq2seqAttentionKeras.call().\n\n\u001b[1mExpected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.cuda.FloatTensor instead (while checking arguments for embedding)\u001b[0m\n\nArguments received by seq2seqAttentionKeras.call():\n  • input_tensor=torch.Tensor(shape=torch.Size([32, 10]), dtype=float32)\n  • training=True'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-122-f51219118fd5>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m history = modelKeras.fit(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0msteps_per_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/backend/torch/trainer.py\u001b[0m in \u001b[0;36m_symbolic_build\u001b[0;34m(self, data_batch)\u001b[0m\n\u001b[1;32m    188\u001b[0m                 \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_output_spec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m                 raise RuntimeError(\n\u001b[0m\u001b[1;32m    191\u001b[0m                     \u001b[0;34m\"Unable to automatically build the model. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m                     \u001b[0;34m\"Please build it yourself before calling \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Unable to automatically build the model. Please build it yourself before calling fit/evaluate/predict. A model is 'built' when its variables have been created and its `self.built` attribute is True. Usually, calling the model on a batch of data is the right way to build it.\nException encountered:\n'Exception encountered when calling seq2seqAttentionKeras.call().\n\n\u001b[1mExpected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.cuda.FloatTensor instead (while checking arguments for embedding)\u001b[0m\n\nArguments received by seq2seqAttentionKeras.call():\n  • input_tensor=torch.Tensor(shape=torch.Size([32, 10]), dtype=float32)\n  • training=True'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tensorflow + Keras"
      ],
      "metadata": {
        "id": "WWF5LZtXcUst"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "import typing\n",
        "from typing import Any, Tuple\n",
        "\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "JSTDTDMFUnbp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderRNN(tf.keras.layers.Layer):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "\n",
        "        # Embedding layer converts tokens to vectors\n",
        "        self.embedding = tf.keras.layers.Embedding(input_size, hidden_size, mask_zero=True)\n",
        "        # RNN layer to process sentence sequentially\n",
        "        self.gru = tf.keras.layers.Bidirectional(\n",
        "            merge_mode='sum',\n",
        "            layer = tf.keras.layers.GRU(hidden_size, return_sequences=True,\n",
        "                                        recurrent_initializer='glorot_uniform'))\n",
        "    def call(self, x):\n",
        "        x = self.embedding(x)\n",
        "        x = self.rnn(x)\n",
        "        return x\n",
        "\n",
        "class CrossAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, hidden_size, **kwargs):\n",
        "        self.mha = tf.keras.layers.MultiHeadAttention(key_dim=hidden_size, num_heads=1, **kwargs)\n",
        "        self.layernorm = tf.keras.layers.LayerNormalization()\n",
        "        self.add = tf.keras.layers.Add()\n",
        "\n",
        "    def call(self, input, context):\n",
        "        attn_output, attn_scores = self.mha(query=input, value=context, return_attention_scores=True)\n",
        "        x = self.add([input, attn_output])\n",
        "        x = self.layernorm(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "class AttentionDecoderRNN(tf.keras.layers.Layer):\n",
        "    def __init__(self, output_size, hidden_size):\n",
        "        super(AttentionDecoderRNN, self).__init__()\n",
        "        # Embedding layer converts tokens to vectors\n",
        "        self.embedding = tf.keras.layers.Embedding(output_size, hidden_size, mask_zero=True)\n",
        "        # RNN layer to process sentence sequentially\n",
        "        self.rnn = tf.keras.layers.GRU(hidden_size, return_sequences=True, return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "        # Decoder RNN output will be the query for the attention layer.\n",
        "        self.attention = CrossAttention(hidden_size)\n",
        "        # Output layer to produce logits\n",
        "        self.output_layer = tf.keras.layers.Dense(output_size)\n",
        "\n",
        "    def call(self, input, context, state=None, return_state=False):\n",
        "        # Lookup the embeddings\n",
        "        x = self.embedding(input)\n",
        "        # Process the target sequence.\n",
        "        x, state = self.rnn(x, initial_state=state)\n",
        "        # Use the RNN output as the query for the attention over the context.\n",
        "        x = self.attention(x, context)\n",
        "        # Generate logit predictions for the next token.\n",
        "        logits = self.output_layer(x)\n",
        "\n",
        "        if return_state:\n",
        "          return logits, state\n",
        "        else:\n",
        "          return logits\n",
        "\n",
        "class Translator(tf.keras.Model):\n",
        "      def __init__(self, input_size, hidden_size, output_size):\n",
        "          super().__init__()\n",
        "          # Build the encoder and decoder\n",
        "          encoder = Encoder(input_size, hidden_size)\n",
        "          decoder = Decoder(output_size, hidden_size)\n",
        "\n",
        "          self.encoder = encoder\n",
        "          self.decoder = decoder\n",
        "\n",
        "      def call(self, inputs):\n",
        "          def call(self, inputs):\n",
        "          context, x = inputs\n",
        "          context = self.encoder(context)\n",
        "          logits = self.decoder(x, context)\n",
        "          return logits\n",
        "\n",
        "      @tf.function\n",
        "      # Compile into a static graph for faster training\n",
        "      def train_step(self, data):\n",
        "          x, y = data\n",
        "\n",
        "          # Forward pass and compute loss\n",
        "          with tf.GradientTape() as tape:\n",
        "              logits = self(x, training=True)\n",
        "              loss_value = self.loss(y, logits)\n",
        "\n",
        "          # Backprop and compute gradients\n",
        "          grads = tape.gradient(loss_value, model.trainable_weights)\n",
        "          # Update weights\n",
        "          self.optimizer.apply(grads, model.trainable_weights)\n",
        "\n",
        "          return loss_value\n",
        "\n",
        "model = Translator(input_size, hidden_size, output_size);\n",
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
        "optimizer = keras.optimizers.Adam(learning_rate=1e-3)\n",
        "\n",
        "model.compile(optimizer=optimizer, loss=loss_fn)\n",
        "\n",
        "history = model.fit(\n",
        "    train_ds.repeat(),\n",
        "    epochs=100,\n",
        "    steps_per_epoch = 100,\n",
        "    validation_data=val_ds,\n",
        "    validation_steps = 20,\n",
        "    callbacks=[tf.keras.callbacks.EarlyStopping(patience=3)])"
      ],
      "metadata": {
        "id": "h-hQ-UIyqKve"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "dced64af9f5c4258a358038847cef2ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_92b6868e4aef4ebca649c15a5fa2c0fb",
              "IPY_MODEL_3c0aee9082c54f6ea6e6eba8b9b2656a",
              "IPY_MODEL_622e5f388cf64bb6be8adb7a73b7bf38"
            ],
            "layout": "IPY_MODEL_3f3911b1383e46879e12ed20f9090a87"
          }
        },
        "92b6868e4aef4ebca649c15a5fa2c0fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e8e47a3959db473b92dd329c251f1ad8",
            "placeholder": "​",
            "style": "IPY_MODEL_e17250f8fccf4fddb9c3fc15a26f391e",
            "value": "Epoch 9: 100%"
          }
        },
        "3c0aee9082c54f6ea6e6eba8b9b2656a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_93237a3f94a9459ca09ea2a62dd39b79",
            "max": 100,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_73f9d0d8a62940c183aa96b3b7b1efa3",
            "value": 100
          }
        },
        "622e5f388cf64bb6be8adb7a73b7bf38": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1a56b5bb8ac3476a94a3160812ead787",
            "placeholder": "​",
            "style": "IPY_MODEL_e5ea797b09a343cb8cf7a3d8633260a3",
            "value": " 100/100 [00:02&lt;00:00, 49.28it/s, v_num=8]"
          }
        },
        "3f3911b1383e46879e12ed20f9090a87": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        },
        "e8e47a3959db473b92dd329c251f1ad8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e17250f8fccf4fddb9c3fc15a26f391e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "93237a3f94a9459ca09ea2a62dd39b79": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "73f9d0d8a62940c183aa96b3b7b1efa3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1a56b5bb8ac3476a94a3160812ead787": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e5ea797b09a343cb8cf7a3d8633260a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}